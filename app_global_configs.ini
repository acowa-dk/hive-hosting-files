#### Acowa Dash Core - global configurations ####
# NOTE:
# - this file should contain all properties available (properties (in section *._GENERAL]) should be as disabled or only some default values) in our product.
# - Properties specific by project/environment should be done in "app_global_configs_custom.ini" file. Properties defined in "app_global_configs_custom.ini" will overwrite default values from default config file (app_global_configs.ini).


# REMEMBER:
# - Keys should NOT contain uppercase letters. The config manager will convert them to lowercase when loading the config.

[XXXX_configs]
key_example=value_example0

[XXXX_configs1]
key_example=value_example1
key_example1=value_example11

[XXXX_configs2]
key_example=value_example2
key_example0=value_example22


[LOG_LEVELS]
modbus_handler_log_level = info
sigfox_input_handler_log_level = info

# THREAD MANAGER CONFIGURATION ################################################
[THREAD_MANAGER_CONFIG]
thread_counter_active = False
# Can be None, normal, verbose
thread_counter_output_type = None

# DATABASE CONFIGURATION ################################################
[DATABASE_CONFIG]
db_connection_string = postgresql://postgres:p0stgr3s@db:5432/appdb
db_log_statements = False
db_psql_path = None

[DATABASE_WORKERS]
enable_db_workers = False

#use case: DeleteOldDataWorker
enable_delete_older_data_worker = False
# keeping data in DB for the last X days (30 days)
retention_period_old_data_days = 30
# used in the first time, after that used the interval in the same time
delete_old_data_start_hour = 1
delete_old_data_interval_days = 1

#  "all" for all or specific device id
delete_older_data_for_gekko_ids =
delete_older_data_for_station_ids =
delete_older_data_for_firefly_ids =
delete_older_data_for_spiders_ids =
delete_older_data_for_pumas_ids =

#use case: move_older_data_for_historical_table
#  all for all or specific device id

#use case: VacuumAndReindexWorker
enable_vacuum_reindex_db_worker = False

# tables for VacuumAndReindexWorker
vacuum_reindex_db_worker_tables = input_gekko.gekko_most_recent, input_scada.station_most_recent, input_firefly.firefly_most_recent, input_puma.puma_most_recent, input_spider.spider_most_recent
# used in the first time, after that used the interval in the same time
vacuum_reindex_db_worker_start_hour = 3
vacuum_reindex_db_worker_interval_days = 7

#use case: run a DB script every X time - to investigate (imagine the scenario in sales.acowa.cloud with file cycle_demo_data.sql)
enable_file_script_worker = False
file_script_location = C:\ACOWA\script\db_script.sql
# used in the first time, after that used the interval in the same time
file_script_worker_start_hour = 4
file_script_worker_interval_days = 1

# UI CONFIGURATION ################################################
[UI_GENERAL]
is_grafana_ui_interface_active = False
hive_ui_active = False

[UI_GRAFANA]
grafana_ui_server_host = 127.0.0.1
grafana_ui_server_port = 65001

[HIVE_UI]
hive_ui_server_host = 127.0.0.1
hive_ui_server_port = 5000

# CALCULATIONS CONFIGURATION ################################################
[CALCULATIONS_GENERAL]
calculations_flow_active = False
calculations_overflow_active = True
parameter_counter_active = False

[CALCULATIONS_FLOW]
flow_repeat_every_x_minutes = 10

# Activate this if there is a lot of noise on pump current data
flow_remove_outliers = False

# activate this to plot station level before and after outlier removal for debugging (opens matplotlib plot)
flow_plot_extras = False

[CALCULATIONS_OVERFLOW]
overflow_repeat_every_x_seconds = 600
is_flow_logging_active = True
use_new_flow_logging_method = False


# INPUTS CONFIGURATIONS ################################################

[INPUTS_GENERAL]
4gdriver_active = False
sigfox_input_active = False
ifix_historian_input_active = False
S2000_data_input_active = False
historian_rest_api_input_active = False
excel_file_input_active = False
rest_api_input_active = False
# scada_get_raw_data_load_logs_active is an alternative to load_scada_station_logs_active + get_scada_raw_data_logs_active deprecated solution
# scada_get_raw_data_load_logs_active should be used when scada logs with deferred time (or delay) or when scada logs the most recent/last timestamp - like an incremental global timestamp
scada_get_raw_data_load_logs_active = False

[INPUTS_4GDRIVER]
4g_run_reverse_com_host = False
4g_host = 0.0.0.0
4g_port = 60616
4g_debug_modbus = False
4g_output_event_log = False
#note: 4g_event_length should be in bytes. (should not be changed because affets code interpretation)
# The number of bytes to expect a Gekko event to be.
4g_event_length = 8
gekko_ai1_max = 10000000
#default: 10 secs (Note: device should be configured with connection timeout > interval reading seconds)
4g_connection_spiders_or_pumas_read_every_x_seconds = 10
#default: 10 minutes (Note: device should be configured with connection timeout > interval reading seconds)
narrowband_connection_spiders_or_pumas_read_every_x_minutes = 10
#Note: for 4G SIM cards (Spiders or Pumas) - NEW PROPERTY (related to the old property 4g_gekko_ids_to_interpret_as_spiders)
4g_connection_gekko_ids_to_interpret_as_spiders_or_pumas=
#Note: for NarrowBand (NB) SIM cards (Pumas and Spiders) - NEW PROPERTY (related to the old property 4g_gekko_ids_to_interpret_as_pumas)
narrowband_connection_gekko_ids_to_interpret_as_spiders_or_pumas =
4g_gekko_ids_using_experimental_read_method =
#NOTE: default should be False because should be activated just when necessary to avoid battery consume
4g_update_device_configs = False
# The number of registers to expect a Gekko user defined logger event to be.
4G_gekko_user_defined_logger_event_length = 22

# Define how often to register spider logs
# Either by percent change or time in seconds
# set to -1 to log at every spider read
# Both can be active at the same time
4g_spider_log_change_pct = -1
4g_spider_log_interval_in_seconds = -1

[INPUTS_SIGFOX]
sigfox_via_callback = False
sigfox_api_usr = 5d9b084b0499f50bc8de7030
sigfox_api_pss = 3980b304f41ed6747078c745ff05ef3c
#how often we should read from Sigfox Backend API, default: 300 secs (5min).
sigfox_api_read_every_x_seconds = 300
sigfox_callback_host = 0.0.0.0
sigfox_callback_port = 50505
sigfox_callback_key = zM54RA9FxsndjVRuDU6UszZB3XPb3jxE
requests_debug_logging = False
firefly_di_1_is_NO = True
firefly_di_2_is_NO = False
firefly_di_1_is_NC_on_ids =
firefly_di_2_is_NO_on_ids =
sigfox_ids_using_pulsecounter =

[INPUTS_IFIX_HISTORIAN]
ifix_historian_repeat_every_x_minutes = 2
#Note: to configure SQL Server connection in INI files (AcowaDashCore)... when the password has special characters (example: AbCd#), they should be duplicated (AbCd##) to be interpreted correctly via module mssql+pymssql
ifix_historian_db_connection_string = mssql+pymssql://admin:admin@127.0.0.1:1433/tests
ifix_historian_db_log_statements = True
ifix_historian_db_first_entry_before_x_hours = 1
ifix_historian_db_query_x_installations = 200

[INPUTS_S2000]
S2000_repeat_every_x_minutes = 2
S2000_get_raw_data_from_sro_connections_via_database_config = False
#Note: to configure SQL Server connection in INI files (AcowaDashCore)... when the password has special characters (example: AbCd#), they should be duplicated (AbCd##) to be interpreted correctly via module mssql+pymssql
S2000_db_connection_string = mssql+pymssql://admin:admin@127.0.0.1:1433/tests
S2000_db_log_statements = False
S2000_db_first_entry_before_x_hours = 4
S2000_db_query_x_installations = 200


# IN DEVELOPMENT PHASE - WORK IN PROGRESS!
[INPUTS_HISTORIAN_REST_API]
historian_rest_api_repeat_every_x_minutes = 10
historian_rest_api_first_entry_before_x_hours = 4
historian_rest_api_query_x_installations = 200
historian_rest_api_url_base=https://<historianservername>:8443/historian-rest-api/v1
historian_rest_api_url_raw_data=${historian_rest_api_url_base}/datapoints/raw/
historian_rest_api_grant_type=client_credentials
historian_rest_api_basic_auth_username=basic_auth_username
historian_rest_api_basic_auth_password=basic_auth_password
historian_rest_api_user=user_tbd
historian_rest_api_pwd=pwd_tbd
historian_rest_api_timeout_secs=5

[INPUTS_EXCEL_FILE]
#Note: Files should be placed in c:/acowa/acowacore_file_import/new_input_files
excel_import_file_path = c:/acowa/acowacore_file_import
excel_file_import_repeat_every_x_seconds = 10

[INPUTS_SCADA_GET_RAW_DATA_LOAD_LOGS]
scada_get_raw_data_load_logs_repeat_every_x_secs = 60
scada_get_raw_data_load_logs_db_query_every_x_installations = 200
scada_get_raw_data_from_sro_connections_via_database_config = False
#Note1: to configure SQL Server connection in INI files (AcowaDashCore)... when the password has special characters (example: AbCd#), they should be duplicated (AbCd##) to be interpreted correctly via module mssql+pymssql
#Note2: sometimes it can be necessary to pass the charset parameter (for instance: in FAXE forsyning project we need to pass the charset="CP936" in the connection string  by adding this ?charset=CP936 to connection string)
scada_get_raw_data_load_logs_db_connection_string = mssql+pymssql://admin:admin@127.0.0.1:1433/tests
scada_get_raw_data_load_logs_db_table_path = [dbo].[IGSS_LOG]
scada_get_raw_data_load_logs_db_log_statements = False
scada_get_raw_data_load_logs_db_first_entry_before_x_hours = 4
# NOTE: MSSQL express with size limitation of 10 Gb (number of days to keep data should be adjusted according).
# Default is disable/false (to avoid automatic delete in a central/shared mssql server)... but if we work with a local mssql server (just for loading logs in acowa core) we should activate in app_global_configs_custom.ini
scada_get_raw_data_load_logs_mssql_delete_old_igss_logs_active = False
scada_get_raw_data_load_logs_mssql_delete_old_igss_logs_before_x_days = 15


# OUTPUTS CONFIGURATION ################################################
[OUTPUTS_GENERAL]
modbus_active = False
csv_active = False
json_active = False
ftp_active = False
sms_active = False
iot_active = False
report_active = False
historian_rest_api_active = False
api_output_active = False

[OUTPUTS_MODBUS]
modbus_host=127.0.0.1
modbus_port=20000
modbus_log_is_debug = False
modbus_write_every_x_secs = 10
modbus_output_to_local_time = True
# gekko id : modbus unit id
modbus_gekko_ids_to_publish =
modbus_firefly_ids_to_publish =
modbus_use_di_counters = False
modbus_distribute_units_by_port = False
modbus_export_data_from_auxiliary = False
modbus_export_data_from_configs = False
modbus_export_historical_data_to_input_registers = False

[OUTPUTS_CSV]
csv_output_to_local_time = False
# just used as initial config to start the export csv handler because later will be used the configured per output in database (when defined)
csv_write_every_x_secs = 60

[OUTPUTS_JSON]
JSON_output_to_local_time = False
JSON_write_every_x_secs = 120

[OUTPUTS_FTP]
ftp_write_every_x_secs = 60

[OUTPUTS_SMS]
sms_forwarding_server_host = 127.0.0.1
sms_forwarding_server_port = 65432
sms_using_new_unified_alerting_syntax_from_grafana_11 = True

[OUTPUTS_IOT]
mqtt_active = False
mqtt_forwarding_server_host = 127.0.0.1
mqtt_forwarding_server_port = 12345

[OUTPUT_REPORT]
report_download_file_path =

[OUTPUTS_HISTORIAN_REST_API]
historian_rest_api_repeat_every_x_minutes = 5
historian_rest_api_url_base=https://<historianservername>:8443/historian-rest-api/v1
historian_rest_api_url_tags=${historian_rest_api_url_base}/tags/
historian_rest_api_user=user_tbd
historian_rest_api_pwd=pwd_tbd
historian_rest_api_timeout_secs=5

[CACHE]
redis_url=redis://cache:6379/0